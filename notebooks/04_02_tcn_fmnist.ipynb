{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6DTRdA_ArKUE"
   },
   "source": [
    "# Fashion-MNIST TCN\n",
    "\n",
    "\n",
    "RNN worflow based on the work of [Aymeric Damien](https://github.com/aymericdamien/TensorFlow-Examples/) and [Sungjoon](https://github.com/sjchoi86/tensorflow-101/blob/master/notebooks/rnn_mnist_simple.ipynb)\n",
    "\n",
    "Good resource: [How to use Dataset in TensorFlow](https://towardsdatascience.com/how-to-use-dataset-in-tensorflow-c758ef9e4428)\n",
    "\n",
    "### Temporal Convolutional Networks Overview\n",
    "\n",
    "![TCNs](https://cdn-images-1.medium.com/max/1000/1*1cK-UEWHGaZLM-4ITCeqdQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UL90ZkmFrKUI"
   },
   "source": [
    "## System Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "cdQrbITIrKUK",
    "outputId": "1bd262f8-b416-4e29-bb82-36172ad05577"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: watermark in /usr/local/lib/python3.5/dist-packages\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.5/dist-packages (from watermark)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.5/dist-packages (from ipython->watermark)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.5/dist-packages (from ipython->watermark)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.5/dist-packages (from ipython->watermark)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.5/dist-packages (from ipython->watermark)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.5/dist-packages (from ipython->watermark)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.5/dist-packages (from ipython->watermark)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.5/dist-packages (from ipython->watermark)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.5/dist-packages (from ipython->watermark)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.5/dist-packages (from ipython->watermark)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.5/dist-packages (from traitlets>=4.2->ipython->watermark)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.5/dist-packages (from traitlets>=4.2->ipython->watermark)\n",
      "Requirement already satisfied: parso==0.1.1 in /usr/local/lib/python3.5/dist-packages (from jedi>=0.10->ipython->watermark)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.5/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->watermark)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.5/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->watermark)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 9.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "q5EsiRJ4rKUU",
    "outputId": "0b0b7ce7-3b1c-408f-c963-e4124e2a680f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.5.2\n",
      "IPython 6.2.1\n",
      "\n",
      "tensorflow 1.5.0\n",
      "numpy 1.14.0\n",
      "\n",
      "compiler   : GCC 5.4.0 20160609\n",
      "system     : Linux\n",
      "release    : 4.10.0-32-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 8\n",
      "interpreter: 64bit\n",
      "Git hash   : 5468040afa72013f062c448adadef29b54276f44\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -m -p tensorflow,numpy -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "SKq6XOmrrKUc"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random \n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irJoB3jnrKUg"
   },
   "source": [
    "## Prepare FMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q7855I0DrKUg",
    "outputId": "e3d4523f-06fd-4c8c-a5d6-e2f6846771b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785) (10000, 785)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../data/fashion-mnist_train.csv\")\n",
    "df_test = pd.read_csv(\"../data/fashion-mnist_test.csv\")\n",
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mJY_ghBhrKUm",
    "outputId": "cba0a8fb-de65-4ce3-ef21-3df58dc650e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['label', 'pixel1', 'pixel2', 'pixel3', 'pixel4', 'pixel5', 'pixel6',\n",
      "       'pixel7', 'pixel8', 'pixel9',\n",
      "       ...\n",
      "       'pixel775', 'pixel776', 'pixel777', 'pixel778', 'pixel779', 'pixel780',\n",
      "       'pixel781', 'pixel782', 'pixel783', 'pixel784'],\n",
      "      dtype='object', length=785)\n",
      "Index(['label', 'pixel1', 'pixel2', 'pixel3', 'pixel4', 'pixel5', 'pixel6',\n",
      "       'pixel7', 'pixel8', 'pixel9',\n",
      "       ...\n",
      "       'pixel775', 'pixel776', 'pixel777', 'pixel778', 'pixel779', 'pixel780',\n",
      "       'pixel781', 'pixel782', 'pixel783', 'pixel784'],\n",
      "      dtype='object', length=785)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.columns)\n",
    "print(df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PFfzc-ZBrKUq",
    "outputId": "9d6544dc-c5d3-46ff-af74-e5338772549f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42394 37741 33781 18995 34876]\n",
      "(10000, 785) (50000, 785)\n"
     ]
    }
   ],
   "source": [
    "# Train/Validation Split\n",
    "idx = np.arange(60000)\n",
    "np.random.shuffle(idx)\n",
    "print(idx[:5])\n",
    "df_val = df_train.iloc[idx[:10000]]\n",
    "df_train = df_train.iloc[idx[10000:]]\n",
    "print(df_val.shape, df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LR1DZ_6GrKUw"
   },
   "source": [
    "## TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mS3hNzCqrKUw"
   },
   "outputs": [],
   "source": [
    "class CausalConv1D(tf.layers.Conv1D):\n",
    "    def __init__(self, filters,\n",
    "               kernel_size,\n",
    "               strides=1,\n",
    "               dilation_rate=1,\n",
    "               activation=None,\n",
    "               use_bias=True,\n",
    "               kernel_initializer=None,\n",
    "               bias_initializer=tf.zeros_initializer(),\n",
    "               kernel_regularizer=None,\n",
    "               bias_regularizer=None,\n",
    "               activity_regularizer=None,\n",
    "               kernel_constraint=None,\n",
    "               bias_constraint=None,\n",
    "               trainable=True,\n",
    "               name=None,\n",
    "               **kwargs):\n",
    "        super(CausalConv1D, self).__init__(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding='valid',\n",
    "            data_format='channels_last',\n",
    "            dilation_rate=dilation_rate,\n",
    "            activation=activation,\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            kernel_constraint=kernel_constraint,\n",
    "            bias_constraint=bias_constraint,\n",
    "            trainable=trainable,\n",
    "            name=name, **kwargs\n",
    "        )\n",
    "       \n",
    "    def call(self, inputs):\n",
    "        padding = (self.kernel_size[0] - 1) * self.dilation_rate[0]\n",
    "        inputs = tf.pad(inputs, tf.constant([(0, 0,), (1, 0), (0, 0)]) * padding)\n",
    "        return super(CausalConv1D, self).call(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "CXQsmAkgrKU2"
   },
   "outputs": [],
   "source": [
    "class TemporalBlock(tf.layers.Layer):\n",
    "    def __init__(self, n_outputs, kernel_size, strides, dilation_rate, dropout=0.2, \n",
    "                 trainable=True, name=None, dtype=None, batch_size=32,\n",
    "                 activity_regularizer=None, **kwargs):\n",
    "        super(TemporalBlock, self).__init__(\n",
    "            trainable=trainable, dtype=dtype,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            name=name, **kwargs\n",
    "        )        \n",
    "        self.dropout = dropout\n",
    "        self.batch_size = tf.cast(batch_size, tf.int32)\n",
    "        self.n_outputs = n_outputs\n",
    "        self.conv1 = CausalConv1D(\n",
    "            n_outputs, kernel_size, strides=strides, \n",
    "            dilation_rate=dilation_rate, activation=tf.nn.relu, \n",
    "            name=\"conv1\")\n",
    "        self.conv2 = CausalConv1D(\n",
    "            n_outputs, kernel_size, strides=strides, \n",
    "            dilation_rate=dilation_rate, activation=tf.nn.relu, \n",
    "            name=\"conv2\")\n",
    "        self.down_sample = None\n",
    "\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        channel_dim = 2\n",
    "        self.dropout1 = tf.layers.Dropout(self.dropout, [self.batch_size, tf.constant(1), tf.constant(self.n_outputs)])\n",
    "        self.dropout2 = tf.layers.Dropout(self.dropout, [self.batch_size, tf.constant(1), tf.constant(self.n_outputs)])\n",
    "        if input_shape[channel_dim] != self.n_outputs:\n",
    "            # self.down_sample = tf.layers.Conv1D(\n",
    "            #     self.n_outputs, kernel_size=1, \n",
    "            #     activation=None, data_format=\"channels_last\", padding=\"valid\")\n",
    "            self.down_sample = tf.layers.Dense(self.n_outputs, activation=None)\n",
    "        self.built = True\n",
    "    \n",
    "    def call(self, inputs, training=True):\n",
    "        x = self.conv1(inputs)\n",
    "        x = tf.contrib.layers.layer_norm(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.conv2(x)\n",
    "        x = tf.contrib.layers.layer_norm(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        if self.down_sample is not None:\n",
    "            inputs = self.down_sample(inputs)\n",
    "        return tf.nn.relu(x + inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PIA3mDWTrKU8"
   },
   "outputs": [],
   "source": [
    "class TemporalConvNet(tf.layers.Layer):\n",
    "    def __init__(self, num_channels, kernel_size=2, dropout=0.2,\n",
    "                 trainable=True, name=None, dtype=None, batch_size=32,\n",
    "                 activity_regularizer=None, **kwargs):\n",
    "        super(TemporalConvNet, self).__init__(\n",
    "            trainable=trainable, dtype=dtype,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            name=name, **kwargs\n",
    "        )\n",
    "        self.layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            out_channels = num_channels[i]\n",
    "            self.layers.append(\n",
    "                TemporalBlock(\n",
    "                    out_channels, kernel_size, strides=1, \n",
    "                    dilation_rate=dilation_size, batch_size=batch_size,\n",
    "                    dropout=dropout, name=\"tblock_{}\".format(i))\n",
    "            )\n",
    "    \n",
    "    def call(self, inputs, training=True):\n",
    "        outputs = inputs\n",
    "        for layer in self.layers:\n",
    "            outputs = layer(outputs, training=training)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUOTkSUnrKVA"
   },
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jMVPkrCGrKVA",
    "outputId": "5f64d2c3-599b-4d56-e29a-4dab332db601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches per epoch: 1563.0\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "display_step = 500\n",
    "print(\"Total batches per epoch:\", np.ceil(df_train.shape[0] / batch_size))\n",
    "training_steps = 10000\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 28 * 28 # timesteps\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.05\n",
    "kernel_size = 8\n",
    "levels = 6\n",
    "nhid = 24 # hidden layer num of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "G2eqnMiZrKVG"
   },
   "outputs": [],
   "source": [
    "def process_batch(batch_x, batch_y):\n",
    "    return tf.expand_dims(batch_x, -1), tf.one_hot(batch_y, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "8A3mzinjrKVK",
    "outputId": "b5c0e794-089d-4c11-c5b7-3d8931b42065"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All parameters: 156129.0\n",
      "Trainable parameters: 52042\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf.set_random_seed(10)\n",
    "    \n",
    "    with tf.variable_scope(\"datasets\"):\n",
    "        training_batch_size = tf.placeholder(tf.int64) # tf.constant(32, dtype=\"int64\")\n",
    "        inference_batch_size = tf.placeholder(tf.int64) # tf.constant(500, dtype=\"int64\")\n",
    "        is_training = tf.placeholder(\"bool\")\n",
    "\n",
    "        fminst_ds_train = tf.data.Dataset.from_tensor_slices(\n",
    "            (df_train.iloc[:, 1:].astype(\"float32\") / 255, df_train.iloc[:, 0].astype(\"int32\"))\n",
    "        ).shuffle(50000, reshuffle_each_iteration=True).repeat().batch(training_batch_size).map(process_batch)\n",
    "        fminst_ds_val = tf.data.Dataset.from_tensor_slices(\n",
    "            (df_val.iloc[:, 1:].astype(\"float32\") / 255, df_val.iloc[:, 0].astype(\"int32\"))\n",
    "        ).repeat().batch(inference_batch_size).map(process_batch)\n",
    "        fminst_ds_test = tf.data.Dataset.from_tensor_slices((\n",
    "            df_test.iloc[:, 1:].astype(\"float32\") / 255, df_test.iloc[:, 0].astype(\"int32\"))\n",
    "        ).repeat().batch(inference_batch_size).map(process_batch)\n",
    "\n",
    "        handle = tf.placeholder(tf.string, shape=[])\n",
    "        iterator = tf.data.Iterator.from_string_handle(\n",
    "            handle, fminst_ds_train.output_types, fminst_ds_train.output_shapes)\n",
    "\n",
    "        train_iterator = fminst_ds_train.make_initializable_iterator()\n",
    "        val_iterator = fminst_ds_val.make_initializable_iterator()\n",
    "        test_iterator = fminst_ds_test.make_initializable_iterator() \n",
    "    \n",
    "    X_0, Y = iterator.get_next()\n",
    "    X = tf.reshape(X_0, (-1, timesteps, num_input))\n",
    "\n",
    "    # Define weights\n",
    "    logits = tf.layers.dense(\n",
    "        TemporalConvNet(\n",
    "            [nhid] * levels, kernel_size, dropout, \n",
    "            batch_size=tf.shape(X)[0])(\n",
    "            X, training=is_training)[:, -1, :],\n",
    "        num_classes, activation=None, \n",
    "        kernel_initializer=tf.orthogonal_initializer()\n",
    "    )\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "   \n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # Define loss and optimizer\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            logits=logits, labels=Y))\n",
    "        ema_loss = tf.get_variable(\"ema_loss\", shape=[], dtype=tf.float32, trainable=False, initializer=tf.constant_initializer(2.5))\n",
    "        ema_update = ema_loss.assign(ema_loss * 0.99 + loss * 0.01)\n",
    "    tf.summary.scalar('Loss', ema_loss)\n",
    "    \n",
    "    with tf.variable_scope(\"optimizer\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        with tf.control_dependencies([ema_update]):\n",
    "            train_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.variable_scope(\"inference\"):\n",
    "        # Evaluate model (with test logits, for dropout to be disabled)\n",
    "        correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "            \n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "    print(\"All parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.global_variables()]))\n",
    "    print(\"Trainable parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.trainable_variables()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kgv2yXvrrKVO"
   },
   "source": [
    "### Training and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "4ZGYvfECrKVO",
    "outputId": "f0b13a92-d552-4dc3-8eeb-414b521a0c5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Train Batch Loss= 3.0748, Training Accuracy= 0.125, Val Accuracy= 0.138\n",
      "Step 500, Train Batch Loss= 0.4752, Training Accuracy= 0.812, Val Accuracy= 0.777\n",
      "Step 1000, Train Batch Loss= 0.2384, Training Accuracy= 0.969, Val Accuracy= 0.826\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Step 1500, Train Batch Loss= 0.5771, Training Accuracy= 0.750, Val Accuracy= 0.835\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Step 2000, Train Batch Loss= 0.5490, Training Accuracy= 0.781, Val Accuracy= 0.851\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Step 2500, Train Batch Loss= 0.1811, Training Accuracy= 0.969, Val Accuracy= 0.854\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Step 3000, Train Batch Loss= 0.3573, Training Accuracy= 0.844, Val Accuracy= 0.862\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Step 3500, Train Batch Loss= 0.0655, Training Accuracy= 1.000, Val Accuracy= 0.870\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Step 4000, Train Batch Loss= 0.4601, Training Accuracy= 0.781, Val Accuracy= 0.854\n",
      "Step 4500, Train Batch Loss= 0.3019, Training Accuracy= 0.875, Val Accuracy= 0.868\n",
      "Step 5000, Train Batch Loss= 0.2728, Training Accuracy= 0.875, Val Accuracy= 0.880\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Step 5500, Train Batch Loss= 0.4950, Training Accuracy= 0.844, Val Accuracy= 0.876\n",
      "Step 6000, Train Batch Loss= 0.2761, Training Accuracy= 0.906, Val Accuracy= 0.880\n",
      "Step 6500, Train Batch Loss= 0.2129, Training Accuracy= 0.938, Val Accuracy= 0.882\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Step 7000, Train Batch Loss= 0.4368, Training Accuracy= 0.812, Val Accuracy= 0.887\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Step 7500, Train Batch Loss= 0.0856, Training Accuracy= 1.000, Val Accuracy= 0.886\n",
      "Step 8000, Train Batch Loss= 0.6618, Training Accuracy= 0.750, Val Accuracy= 0.880\n",
      "Step 8500, Train Batch Loss= 0.2045, Training Accuracy= 0.906, Val Accuracy= 0.887\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Step 9000, Train Batch Loss= 0.3387, Training Accuracy= 0.875, Val Accuracy= 0.886\n",
      "Step 9500, Train Batch Loss= 0.1921, Training Accuracy= 0.938, Val Accuracy= 0.886\n",
      "Step 10000, Train Batch Loss= 0.1805, Training Accuracy= 0.906, Val Accuracy= 0.893\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Test Accuracy= 0.895\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "best_val_acc = 0.8\n",
    "\n",
    "train_writer = tf.summary.FileWriter(\"logs/fminst_tcn/%s/train\" % datetime.now().strftime(\"%Y%m%d_%H%M\"), graph)\n",
    "val_writer = tf.summary.FileWriter(\"logs/fminst_tcn/%s/val\" % datetime.now().strftime(\"%Y%m%d_%H%M\"))\n",
    "\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    # Run the initializer    \n",
    "    sess.run(init) \n",
    "    sess.run([train_iterator.initializer, val_iterator.initializer, test_iterator.initializer],\n",
    "             feed_dict={training_batch_size: batch_size, inference_batch_size: 500})\n",
    "    train_handle, val_handle, test_handle = sess.run(\n",
    "        [train_iterator.string_handle(), val_iterator.string_handle(), test_iterator.string_handle()])\n",
    "    for step in range(1, training_steps+1):\n",
    "        sess.run([train_op], feed_dict={handle: train_handle, is_training: True})\n",
    "        \n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss_local, acc, summary = sess.run(\n",
    "                [loss, accuracy, merged_summary_op], \n",
    "                feed_dict={handle: train_handle, is_training: False})\n",
    "            train_writer.add_summary(summary, global_step=step)\n",
    "            train_writer.flush()            \n",
    "            val_acc, val_loss = [], []\n",
    "            for _ in range(20):\n",
    "                tmp = sess.run(\n",
    "                    [accuracy, loss], \n",
    "                    feed_dict={handle: val_handle, is_training: False})\n",
    "                val_acc.append(tmp[0])\n",
    "                val_loss.append(tmp[1])\n",
    "            summary = tf.Summary()\n",
    "            val_loss = np.mean(val_loss)            \n",
    "            val_acc = np.mean(val_acc)            \n",
    "            summary.value.add(tag='Loss', simple_value=val_loss)\n",
    "            val_writer.add_summary(summary, global_step=step)\n",
    "            val_writer.flush() \n",
    "            print(\"Step \" + str(step) + \", Train Batch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss_local) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc) + \", Val Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(val_acc))\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
    "                print(\"Model saved in path: %s\" % save_path)  \n",
    "    test_acc = []\n",
    "    for _ in range(20):\n",
    "        test_acc.append(sess.run(accuracy, feed_dict={handle: test_handle, is_training: False}))\n",
    "    test_acc = np.mean(test_acc)\n",
    "    print(\"Test Accuracy= {:.3f}\".format(test_acc))                \n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "train_writer.close()\n",
    "val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gKbqF6g0rKVe",
    "outputId": "7b32bec9-232a-418f-84cc-d9f11dbc7c1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Test Accuracy= 0.895\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    saver.restore(sess, '/tmp/model.ckpt')\n",
    "    sess.run(test_iterator.initializer,\n",
    "             feed_dict={inference_batch_size: 500})\n",
    "    test_handle = sess.run(test_iterator.string_handle())    \n",
    "    test_acc = []\n",
    "    for _ in range(20):\n",
    "        test_acc.append(sess.run(accuracy, feed_dict={handle: test_handle, is_training: False}))\n",
    "    test_acc = np.mean(test_acc)\n",
    "    print(\"Test Accuracy= {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vtsbz-3_rKVm"
   },
   "source": [
    "## Permute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "W2s8jtiPrKVm"
   },
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "training_steps = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "oPbaMIBerKVo",
    "outputId": "a23ec18c-a364-440d-d0f6-44d6f38402cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All parameters: 156129.0\n",
      "Trainable parameters: 52042\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf.set_random_seed(10)\n",
    "    \n",
    "    with tf.variable_scope(\"datasets\"):\n",
    "        training_batch_size = tf.placeholder(tf.int64) # tf.constant(32, dtype=\"int64\")\n",
    "        inference_batch_size = tf.placeholder(tf.int64) # tf.constant(500, dtype=\"int64\")\n",
    "        is_training = tf.placeholder(\"bool\")\n",
    "\n",
    "        fminst_ds_train = tf.data.Dataset.from_tensor_slices(\n",
    "            (df_train.iloc[:, 1:].astype(\"float32\") / 255, df_train.iloc[:, 0].astype(\"int32\"))\n",
    "        ).shuffle(50000, reshuffle_each_iteration=True).repeat().batch(training_batch_size).map(process_batch)\n",
    "        fminst_ds_val = tf.data.Dataset.from_tensor_slices(\n",
    "            (df_val.iloc[:, 1:].astype(\"float32\") / 255, df_val.iloc[:, 0].astype(\"int32\"))\n",
    "        ).repeat().batch(inference_batch_size).map(process_batch)\n",
    "        fminst_ds_test = tf.data.Dataset.from_tensor_slices((\n",
    "            df_test.iloc[:, 1:].astype(\"float32\") / 255, df_test.iloc[:, 0].astype(\"int32\"))\n",
    "        ).repeat().batch(inference_batch_size).map(process_batch)\n",
    "\n",
    "        handle = tf.placeholder(tf.string, shape=[])\n",
    "        iterator = tf.data.Iterator.from_string_handle(\n",
    "            handle, fminst_ds_train.output_types, fminst_ds_train.output_shapes)\n",
    "\n",
    "        train_iterator = fminst_ds_train.make_initializable_iterator()\n",
    "        val_iterator = fminst_ds_val.make_initializable_iterator()\n",
    "        test_iterator = fminst_ds_test.make_initializable_iterator() \n",
    "    \n",
    "    X_0, Y = iterator.get_next()\n",
    "    \n",
    "    # Permute the time step\n",
    "    np.random.seed(100)\n",
    "    permute = np.random.permutation(784)\n",
    "    X = tf.gather(\n",
    "        tf.reshape(X_0, (-1, timesteps, num_input)), \n",
    "        permute, axis=1)\n",
    "\n",
    "    # Define weights\n",
    "    logits = tf.layers.dense(\n",
    "        TemporalConvNet([nhid] * levels, kernel_size, dropout)(\n",
    "            X, training=is_training)[:, -1, :],\n",
    "        num_classes, activation=None, \n",
    "        kernel_initializer=tf.orthogonal_initializer()\n",
    "    )\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "   \n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # Define loss and optimizer\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            logits=logits, labels=Y))\n",
    "        ema_loss = tf.get_variable(\"ema_loss\", shape=[], dtype=tf.float32, trainable=False, initializer=tf.constant_initializer(2.5))\n",
    "        ema_update = ema_loss.assign(ema_loss * 0.99 + loss * 0.01)\n",
    "    tf.summary.scalar('Loss', ema_loss)\n",
    "    \n",
    "    with tf.variable_scope(\"optimizer\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        with tf.control_dependencies([ema_update]):\n",
    "            train_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.variable_scope(\"inference\"):\n",
    "        # Evaluate model (with test logits, for dropout to be disabled)\n",
    "        correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "            \n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "    print(\"All parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.global_variables()]))\n",
    "    print(\"Trainable parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.trainable_variables()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "CJaoPVourKVu",
    "outputId": "f8f4265e-8dcb-473e-b9c1-afbcfad0a848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Train Batch Loss= 3.6829, Training Accuracy= 0.094, Val Accuracy= 0.151\n",
      "Step 500, Train Batch Loss= 0.4500, Training Accuracy= 0.781, Val Accuracy= 0.808\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Step 1000, Train Batch Loss= 0.3328, Training Accuracy= 0.875, Val Accuracy= 0.826\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Step 1500, Train Batch Loss= 0.5785, Training Accuracy= 0.750, Val Accuracy= 0.836\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Step 2000, Train Batch Loss= 0.7037, Training Accuracy= 0.719, Val Accuracy= 0.842\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Step 2500, Train Batch Loss= 0.2977, Training Accuracy= 0.875, Val Accuracy= 0.852\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Step 3000, Train Batch Loss= 0.5172, Training Accuracy= 0.812, Val Accuracy= 0.856\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Step 3500, Train Batch Loss= 0.0847, Training Accuracy= 1.000, Val Accuracy= 0.862\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Step 4000, Train Batch Loss= 0.4534, Training Accuracy= 0.812, Val Accuracy= 0.852\n",
      "Step 4500, Train Batch Loss= 0.4209, Training Accuracy= 0.844, Val Accuracy= 0.857\n",
      "Step 5000, Train Batch Loss= 0.3004, Training Accuracy= 0.938, Val Accuracy= 0.866\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Step 5500, Train Batch Loss= 0.5370, Training Accuracy= 0.812, Val Accuracy= 0.866\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Step 6000, Train Batch Loss= 0.1609, Training Accuracy= 0.969, Val Accuracy= 0.865\n",
      "Step 6500, Train Batch Loss= 0.3188, Training Accuracy= 0.906, Val Accuracy= 0.868\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Step 7000, Train Batch Loss= 0.5024, Training Accuracy= 0.812, Val Accuracy= 0.868\n",
      "Step 7500, Train Batch Loss= 0.1029, Training Accuracy= 0.969, Val Accuracy= 0.868\n",
      "Step 8000, Train Batch Loss= 0.6994, Training Accuracy= 0.781, Val Accuracy= 0.875\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Step 8500, Train Batch Loss= 0.2333, Training Accuracy= 0.938, Val Accuracy= 0.873\n",
      "Step 9000, Train Batch Loss= 0.3785, Training Accuracy= 0.875, Val Accuracy= 0.868\n",
      "Step 9500, Train Batch Loss= 0.1423, Training Accuracy= 0.938, Val Accuracy= 0.873\n",
      "Step 10000, Train Batch Loss= 0.1809, Training Accuracy= 0.969, Val Accuracy= 0.881\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Step 10500, Train Batch Loss= 0.6229, Training Accuracy= 0.812, Val Accuracy= 0.873\n",
      "Step 11000, Train Batch Loss= 0.2296, Training Accuracy= 0.875, Val Accuracy= 0.876\n",
      "Step 11500, Train Batch Loss= 0.1517, Training Accuracy= 0.938, Val Accuracy= 0.880\n",
      "Step 12000, Train Batch Loss= 0.4488, Training Accuracy= 0.875, Val Accuracy= 0.876\n",
      "Step 12500, Train Batch Loss= 0.1679, Training Accuracy= 0.938, Val Accuracy= 0.879\n",
      "Step 13000, Train Batch Loss= 0.0961, Training Accuracy= 0.969, Val Accuracy= 0.879\n",
      "Step 13500, Train Batch Loss= 0.2742, Training Accuracy= 0.906, Val Accuracy= 0.882\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Step 14000, Train Batch Loss= 0.3210, Training Accuracy= 0.875, Val Accuracy= 0.882\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "Step 14500, Train Batch Loss= 0.1791, Training Accuracy= 0.969, Val Accuracy= 0.878\n",
      "Step 15000, Train Batch Loss= 0.3312, Training Accuracy= 0.875, Val Accuracy= 0.880\n",
      "Test Accuracy= 0.881\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "best_val_acc = 0.8\n",
    "\n",
    "train_writer = tf.summary.FileWriter(\"logs/fminst_tcn/%s_permute/train\" % datetime.now().strftime(\"%Y%m%d_%H%M\"), graph)\n",
    "val_writer = tf.summary.FileWriter(\"logs/fminst_tcn/%s_permute/val\" % datetime.now().strftime(\"%Y%m%d_%H%M\"))\n",
    "\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    sess.run(init) \n",
    "    sess.run([train_iterator.initializer, val_iterator.initializer, test_iterator.initializer],\n",
    "             feed_dict={training_batch_size: batch_size, inference_batch_size: 500})\n",
    "    train_handle, val_handle, test_handle = sess.run(\n",
    "        [train_iterator.string_handle(), val_iterator.string_handle(), test_iterator.string_handle()])\n",
    "    # Run the initializer\n",
    "    for step in range(1, training_steps+1):\n",
    "        sess.run([train_op], feed_dict={handle: train_handle, is_training: True})\n",
    "        \n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss_local, acc, summary = sess.run(\n",
    "                [loss, accuracy, merged_summary_op], \n",
    "                feed_dict={handle: train_handle, is_training: False})\n",
    "            train_writer.add_summary(summary, global_step=step)\n",
    "            train_writer.flush()            \n",
    "            val_acc, val_loss = [], []\n",
    "            for _ in range(20):\n",
    "                tmp = sess.run(\n",
    "                    [accuracy, loss], \n",
    "                    feed_dict={handle: val_handle, is_training: False})\n",
    "                val_acc.append(tmp[0])\n",
    "                val_loss.append(tmp[1])\n",
    "            summary = tf.Summary()\n",
    "            val_loss = np.mean(val_loss)            \n",
    "            val_acc = np.mean(val_acc)            \n",
    "            summary.value.add(tag='Loss', simple_value=val_loss)\n",
    "            val_writer.add_summary(summary, global_step=step)\n",
    "            val_writer.flush() \n",
    "            print(\"Step \" + str(step) + \", Train Batch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss_local) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc) + \", Val Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(val_acc))\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
    "                print(\"Model saved in path: %s\" % save_path)  \n",
    "    test_acc = []\n",
    "    for _ in range(20):\n",
    "        test_acc.append(sess.run(accuracy, feed_dict={handle: test_handle, is_training: False}))\n",
    "    test_acc = np.mean(test_acc)\n",
    "    print(\"Test Accuracy= {:.3f}\".format(test_acc))                \n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "train_writer.close()\n",
    "val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "aCbMj_vKrKV2",
    "outputId": "22bd5a33-daba-4ef6-8805-81bc1db01c44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Test Accuracy= 0.881\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    saver.restore(sess, '/tmp/model.ckpt')\n",
    "    sess.run(test_iterator.initializer,\n",
    "             feed_dict={inference_batch_size: 500})\n",
    "    test_handle = sess.run(test_iterator.string_handle())    \n",
    "    test_acc = []\n",
    "    for _ in range(20):\n",
    "        test_acc.append(sess.run(accuracy, feed_dict={handle: test_handle, is_training: False}))\n",
    "    test_acc = np.mean(test_acc)\n",
    "    print(\"Test Accuracy= {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dcj2V2ifrKV4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "tcn_fmnist.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
